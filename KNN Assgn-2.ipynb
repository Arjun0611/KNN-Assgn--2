{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ecc3320-d1dc-418b-9faf-60146b07400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1.\n",
    "\n",
    "# Euclidean vs Manhattan Distance in KNN:\n",
    "\n",
    "# Euclidean Distance:\n",
    "# Measures straight-line distance between points.\n",
    "# Sensitive to magnitudes of features.\n",
    "# Works well with istropic (uniformly shaped) clusters.\n",
    "\n",
    "# Manhattan Distance:\n",
    "# Measures distance along gridlines (city-block distance).\n",
    "# Less sensitive to feature magnitudes.\n",
    "# Better suited for high-dimensional or sparse data.\n",
    "\n",
    "# Impact on KNN Performance:\n",
    "# Feature Magnitudes: Euclidean is sensitive; Manhattan less so.\n",
    "# Curse of Dimensionality: Manhattan may perform better in high-dimensional spaces.\n",
    "# Data Characteristics: Euclidean may suit isotropic clusters; Manhattan may handle elongated clusters better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86d4213d-d379-4e82-bd60-345bc096221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2.\n",
    "\n",
    "# Choosing Optimal k in KNN:\n",
    "\n",
    "# Grid Search:\n",
    "# Evaluate performance for various k values.\n",
    "# Select the k yielding the best results.\n",
    "\n",
    "# Cross-Validation:\n",
    "# Use k-fold cross-validation to assess performance.\n",
    "# Identify k with the lowest cross-validation error.\n",
    "\n",
    "# Rule of Thumb:\n",
    "# For small datasets, k = sqrt(n)\n",
    "# Adjust based on dataset characteristics.\n",
    "\n",
    "# Considerations:\n",
    "# Odd vs. Even k: Prefer odd for binary classification to avoid ties.\n",
    "# Impact of Outliers: Adjust k based on sensitivity to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ccbc84c-7cac-4ab6-bd6c-0d080a689e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.\n",
    "\n",
    "#Impact of Distance Metric in KNN:\n",
    "\n",
    "#Euclidean Distance:\n",
    "\n",
    "#Measures straight-line distance.\n",
    "#Sensitive to scale differences among features.\n",
    "#Suitable for features with similar importance.\n",
    "\n",
    "#Manhattan Distance:\n",
    "\n",
    "#Measures sum of absolute differences.\n",
    "#Less sensitive to scale variations.\n",
    "#Appropriate when features have unequal importance.\n",
    "\n",
    "# Choosing Distance Metric:\n",
    "\n",
    "# Feature Characteristics:\n",
    "# Assess feature scales and importance.\n",
    "# Use Euclidean for balanced features; Manhattan for diverse importance.\n",
    "\n",
    "# Data Distribution:\n",
    "# Examine data distribution.\n",
    "# Opt for Manhattan if outliers or uneven feature importance.\n",
    "\n",
    "# Computational Efficiency:\n",
    "# Consider computational cost.\n",
    "# Manhattan is computationally less expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb879a0b-dae9-4222-9e53-b14b17eab7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4.\n",
    "\n",
    "# Common Hyperparameters in KNN:\n",
    "\n",
    "#1. K (Number of Neighbors):\n",
    "# Determines the number of neighbors considered.\n",
    "# Higher K leads to smoother decision boundaries.\n",
    "\n",
    "# Distance Metric:\n",
    "# Defines the measure for similarity between instances.\n",
    "# Options include Euclidean, Manhattan, etc.\n",
    "\n",
    "# Weighting Scheme:\n",
    "# Specifies how neighbor contributions are weighted.\n",
    "# Options include uniform or distance-based weighting.\n",
    "\n",
    "# Impact on Model:\n",
    "\n",
    "#K:\n",
    "# Lower K increases model complexity.\n",
    "# Higher K may lead to oversmoothing.\n",
    "\n",
    "# Distance Metric:\n",
    "# Choice affects sensitivity to feature scales.\n",
    "\n",
    "# Weighting Scheme:\n",
    "# Weighting by distance gives closer neighbors more influence.\n",
    "\n",
    "# Hyperparameter Tuning:\n",
    "\n",
    "# Grid Search:\n",
    "# Systematically test combinations.\n",
    "# Evaluate performance using cross-validation.\n",
    "\n",
    "# Random Search:\n",
    "# Explore hyperparameter space randomly.\n",
    "# Efficient for large search spaces.\n",
    "\n",
    "# Domain Knowledge:\n",
    "# Leverage undersanding of data and problem.\n",
    "# Guide hyperparameter choices accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23fdcffc-2fc2-45d2-bbdc-d429ffe79194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5.\n",
    "\n",
    "# Effect of Training Set Size of KNN:\n",
    "\n",
    "# Small Training Set:\n",
    "# Prone to overfitting due to noise senitivity.\n",
    "# Decision boundaries may be jagged.\n",
    "\n",
    "# Large Training Set:\n",
    "# Reduces overfitting, provides more representative patterns.\n",
    "# Comuptationally expensive for prediction.\n",
    "\n",
    "# Optimizing Training Set Size:\n",
    "\n",
    "# Cross-Validation:\n",
    "# Use cross-validation to assess performance.\n",
    "# Identify a balance b/w bias and variance.\n",
    "\n",
    "# Learning Curves:\n",
    "# Plot learning curves to visualize performance.\n",
    "# Observe how metrics change with increasing data.\n",
    "\n",
    "# Incremental Learning:\n",
    "# Implement techniques for incremental learning.\n",
    "# Efficiently update models as new data arrives.\n",
    "\n",
    "# Feature Importance:\n",
    "# Identify influential features.\n",
    "# Focus on collecting data for critical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e8b0514-ca1e-43e8-9431-54615c08cd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6.\n",
    "\n",
    "# Drawbacks of KNN and how to overcome them.\n",
    "\n",
    "# Computational Cost:\n",
    "# Issue: Computationally expensive during prediction, especially with large datasets.\n",
    "# Mitigation: Implement optimizations like KD-trees or Ball trees for efficient search.\n",
    "\n",
    "# Curse of Dimensionality:\n",
    "# Issue: Performance degrades with high-dimensional data due to increased sparsity.\n",
    "# Mitigation: Use feature selection, dimensionality reduction, or other distance metrics.\n",
    "\n",
    "# Sensitive to Outliers:\n",
    "# Issue: Prone to noise and outliers affecting predictions.\n",
    "# Mitigation: Apply outlier detection techniques or consider robust distance metrics.\n",
    "\n",
    "# Biases by Majority Class:\n",
    "# Issue: Biased toward the majority class in imbalanced datasets.\n",
    "# Mitigation: Implement techniques like oversampling, undersampling, or using modified distance metrics.\n",
    "\n",
    "# Choice of Distance Metric:\n",
    "# Issue: Performance heavily depends on the chosen distance metric.\n",
    "# Mitigation: Experiment with different distance metrics based on data characteristics.\n",
    "\n",
    "# Need for Feature Scaling:\n",
    "# Issue: Sensitivity to feature scales affecting distance calculations.\n",
    "# Mitigation: Normalize or standardize features to bring them to a similar scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec4bfd9-352c-4c68-87c7-08af0be9864a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
